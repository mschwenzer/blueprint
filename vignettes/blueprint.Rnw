---
title: "Introduction to blueprint"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
%\VignetteIndexEntry{Introduction to blueprint}
%\VignetteEngine{knitr::rmarkdown}
\usepackage[utf8]{inputenc}
---


# Overview

Modern data users face the need to combine very different data sources which can easily become confusing. The R-package blueprint offers the opportunity to select, combine and transform data in an easy and intuitive way by using a meta-data file (a "blueprint") that can be easily edited with a chosen Spreadsheet application or text editor capable of e.g. saving the file formats OpenOffice Calc, Excel, CSV, HTML. Merging variables that are split over dozens of different files is done by entering the absolute necessary information into a text-based meta-file and let the package do the rest. Blueprints define which variables from files are imported and (left) joins variables from other files. Blueprint is also best suited for the needs of joining longitudinal data and unifying variable names that changed across different waves into one new variable. By relying on the versatile rio-package\ for file import it offers the opportunity to handle very different kind of data formats (for the imported data as well as the blueprint-files.). Every variable can be transformed by functions or a chain of functions (operations seqentially applied to the original variable from the corresponding file /before/ it is merged into the final data frame). Repetitive recoding of similar variables becomes very easy. By relying on meta-files data-processing becomes more versatile, clearer and more easy expandable. The workhorse-functions that merge and transform data in the background rely mainly on functions from the brilliant and comparable fast packages dplyr and data.table (?CITE). 

# Merge same cases from different files

In blueprint the merge process is specified by a so-called blueprint file into which we enter data in a standard Spreadsheet-Application like Excel or Open Office Spreadsheet. To enter data, we first have to create a new blueprint template. This can be done easily by entering:

```{r}
open.blue('my.blueprint.name.csv')
```

Alternatively we could change the suffix of the filename to xlsx to open an Excel-File:

```{r}
open.blue('my.blueprint.name.xlsx) \\
```

Every row represents an unique variable of a final data.frame. The name of this variable can be chosen according to the needs of the analysis that is specified in column one ('newvar'). Assume we have 2 files 'individual.csv' and 'household.csv' and want to import the variables '/idno/, '/gender/' and '/birthY/' from file 'individual.csv'. This original variable names have to be written into the column var1 like in figure X. 

| newvar             | var1     | file1          | link1             | fun1 |
|------------------- | ---------| -------------- | ------------------|------|
| individual.id      | idno     | individual.csv |                   |      |
| gender             | gend     | individual.csv |                   |      |
| birth.year         | birthY   | individual.csv |                   |      |
| household.id       | houseID  | individual.csv |                   |      |
| ###                |          |                |                   |      |
| household.income   | income   | household.csv  | household.id=idno |      |
| household.property | property | household.csv  | household.id=idno |      |

Adding the additional variable '/household income/' and '/property/' from the seperate file `household.csv` typically includes left joining them, which equals to adding columns for additional variables and keeping only the values of household.csv of those units that are also in individual.csv. For this we have to define conditions which rows in the files individual.csv and household.csv relate to the same unit. This is achieved by specifiying the column link for the files to add from. Relying on the syntax of /dplyr::left_join/ (while leaving out apostrophes) the condition expresses which variables must be equal: You give the new variable name (specified in newvar) and the name of the matching old variable name in the original data file that is to be added: /name.in.newvar=old.variable.name.in.data.file.to.merge/
Assuming e.g. that in file 'household.csv' every household has an id number called '/idno/' we would specify the link condition /household.id=idno/. Several link conditions can be combined by comma. The first part relates to new variable name specified as '/newvar/', the second to the original name in the data file to merge. If only single variables are given, it is assumed that the variable names are the same (name of newvar and var in a file to add are identical).
If the variables used inside of the link condition are not in the /var1.../ column they will be used for merging but dropped afterwards.

When a blueprint has been modified and saved in a external application, the merged data can be imported into R by simply stating:

```{r}
my.df <- blue('my.blueprint.name.csv')\\
```
/my.df/ is a new data.frame with the selected variables from individual.csv plus the household variables that match the link condition.

# Transforming variables by functions
By now the column fun1 has been empty. This column offers a convenient way to transform variables (e.g. for recoding categories). The specified functions are executed with the original variable which will be automatically replaced by the result of the function. Let's say we want to recode gender from /0/ to '/Male/' and from /1/ to '/Female/'. You could do this by using recode from the dplyr package like shown in figure XXX (leaving out the first argument x (variable name) which the original variable will be inserted automatically). The result of this blueprint will be a data.frame with gender containing the values '/Male/' and '/Female/'. Note that a fun entry can be used to execute a whole chain of transformations (seperated by '%>%'). Have a look at the documentation on pipes provided by /magrittr/ how to use pipes /?magrittr::%>%/. Also note that e.g. recoding  different variables with the same coding scheme can be done by copy and paste of fields of fun1 to other rows.

# Merging different waves
Repeated surveys typically are delivered as so-called waves. Blueprint makes it easy to merge different waves into a long format. Independent of whether the same units are measured repeatedly or the same variables are measured repeatedly for different units, the purpose will be to combine data /rowwise/. Data providers (like e.g. OECD PISA) sometimes emit wave-files with changed variable names for the same items. Harmonizing this variable is very easy with using blueprint. The /newvar/ column contains the variable names. Additional variable columns starting with var (e.g. /var1/, /var2/,...) contain the variable names from the files specific to the wave. We can join two waves by entering additional columns containing the identifier var,file,link,fun. Or we can initialize a new blueprint with the waves argument using:
\begin{tt}
open.blue('my.blueprint.name2.xlsx,waves=1:2) \\
\\
\end{tt}
Figure XXX shows a more advandced blueprint file reflecting this structure. 

|-------------+----------+----------+------------+------+---------+------------+-------+------------|
| newvar      | var1     | file1    | link1      | fun1 | var2    | file2      | link2 | fun2       |
|-------------+----------+----------+------------+------+---------+------------+-------+------------|
| i.id        | idno     | i.w1.dta |            |      | ID      | i.w2.Rdata |       |            |
| gender      | gend     | i.w1.dta |            |      | gend    | i.w2.Rdata |       |            |
| birth.year  | birthY   | i.w1.dta |            |      | birth   | i.w2.Rdata |       |            |
| hh.id       | houseID  | i.w1.dta |            |      | houseID | iw.2.Rdata |       |            |
| ###         |          |          |            |      |         |            |       |            |
| hh.income   | income   | h.w2.dta | hh.id=idno |      |         | h.w2.csv   |       | hh.id=idno |
| hh.property | property | h.w2.dta | hh.id=idno |      |         | h.w2.csv   |       | hh.id=idno |
|-------------+----------+----------+------------+------+---------+------------+-------+------------|

Entering the appropriate data will rename, transform and join the data automatically. In short: Waves are specified /columnwise/ (with blocks of 4 columns for each wave containing the original variable name, filepath, link conditions, and transformation functions). Columns that relate to units in the same wave are specified /rowwise/ by giving different names and setting the link condition.

# Source code
Blueprint by default creates a corresponding .R code file containing all operations necessary to merge 
the data, by default written to /filepath_without_extension.blueprint.code.R/. If you choose to do so you can use, edit and share this standalone file without the need to have the blueprint package installed. 
# Logging and descriptives of the merging / transfomation process

Blueprint is constructed to not be very verbose when called. Nonetheless it has a logging feature that can be activated by setting /blue(...,logfile=TRUE)/. In this case a extended logfile is created that contains also information on the transformation process (recode table, descriptives, distribution and information about automatic type conversions, statistics on dimension of the data). The parameter /logfile/ is set to /FALSE/ by default, only standard information will be written to this file. If you set logfile to a character string (by e.g. /blue(...,logfile='a.file.txt')/), an extended logfile will be written to this path. If you don't specify a path, the name will be resembled by the name of the blueprint file The computation of the statistics take some time and therefore there is a tradeoff between time and the comfort of additional information.  

```
----Transformation. Variable `ST03Q01`  (wave 1): recode(`2`=0L,`1`=1L,.default=NA_integer_)  ----------------------------- 
  
  ============================== 
  old    1      2     7   8   9 
  ..     |      |     |   |   | 
  ...    v      v     v   v   v 
  new    1      0 
  X.n. 115030 112128 1055 15 556 
  ------------------------------ 
 !!! Type conversion from numeric to integer. Was this intended? 
  
  
 >>> Distribution after recoding ----- 
 variable 
  n missing  unique    Info     Sum    Mean 
  227158    1626       2    0.75  115030  0.5064 
```

# Assigning fixed value
It might be convenient to create new variables that are constant for every unit of the same wave. This can be done by entering names  for var1 , var2,... that are not in the original data file. To assign fixed value to a new variable you either use the integer specification or encapsulating characters into apostrophe (') . Note that since Excel has a special treatment of captioning characters using two beginning Apostrophes and one ending apostrophe probably will have to be used  (''character value') .
|-------------+-------------+----------+-------|
| newvar      | var1        | file1    | link1 |
|-------------+-------------+----------+-------|
| i.id        | idno        | i.w1.dta |       |
| survey.year | 2000L       | i.w1.dta |       |
| wavec       | 'PISA2000'  | i.w1.dta |       |
|-------------+-------------+----------+-------|
Note the difference between e.g. /i.id/ stemming from a column in the file i.w1.dta and /survey.year/ which will be the same ("PISA2000") for all units in file i.w1.dta.

# Selection of multiple variables
Assume you have 80 weight variables specified by rep.weight1 to rep.weight80. You can specify these in var1 as /rep.weight[1:80]/. The rows containing brackets will be expanded to 80 additional rows with the specific name, resulting in the import (and if specified also individual transformation) of all of this variables.

# File formats for importing and exporting.
When importing files the package /blueprint/ relies on the function /import/ from the package /rio/. This package recognises the most frequently used file formats based on their corresponding suffix as listed in Figure XXX. This applies to loading blueprint files as well as data file specified in this data files.
| Suffix                     | Assumed file format |
|----------------------------+---------------------|
| R binary file              | .Rdata              |
| Stata files                | .rda                |
| SPSS files                 | .sav                |
| Comma seperated Text files | .csv                |
| HTML files                 | .html               |
| Excel files                | .xlsx               |
| Open Spreadsheet files     | .ods                |
|----------------------------+---------------------|
Note that additional arguments of blue are postponed to the function import resulting in additional specifications for import process. If you e.g. want to import the specific spreadsheet "MyData" in a Excel file (default would be the first Spreadsheet) it can be selected by specifying a /which/ argument.
/blue('/path/to/blueprint.file.xlx',which='MyData')/

# Exporting files
By giving the argument /export.file/, e.g. blue(..., export.file='/path/to/file.csv') the merged data.frame will be written directly to the file specified. In this case the data.frame will be returned /invisible/ - it can be used in other functions or pipes, but will not be printed and automatically deleted frome memory if not assigned to a new specifier.

# Aggregating data
Will follow.

# Blatest

The idea behind **rio** is to simplify the process of importing data into R and exporting data from R. This process is, probably unnecessarily, extremely complex for beginning R users. Indeed, R supplies [an entire manual](https://cran.r-project.org/doc/manuals/r-release/R-data.html) describing the process of data import/export. And, despite all of that text, most of the packages described are (to varying degrees) out-of-date. Faster, simpler, packages with fewer dependencies have been created for many of the file types described in that document. **rio** aims to unify data I/O (importing and exporting) into two simple functions: `import` and `export` so that beginners (and experienced R users) never have to think twice (or even once) about the best way to read and write R data.

The core advantage of **rio** is that it makes assumptions that the user is probably willing to make. Specifically, **rio** uses the file extension of a file name to determine what kind of file it is. This is the same logic used by Windows OS, for example, in determining what application is associated with a given file type. By taking away the need to manually match a file type (which a beginner may not recognize) to a particular import or export function, **rio** allows almost all common data formats to be read with the same function.

By making import and export easy, it's an obvious next step to also use R as a simple data conversion utility. Transferring data files between various proprietary formats is always a pain and often expensive. The `convert` function therefore combines `import` and `export` to easily convert between file formats (thus providing a FOSS replacement for programs like [Stat/Transfer](https://www.stattransfer.com/) or [Sledgehammer](https://www.mtna.us/#/products/sledgehammer)).

## Supported file formats ##

**rio** supports a variety of different file formats for import and export.

| Format | Import | Export |
| ------ | ------ | ------ |
| Tab-separated data (.tsv) | Yes | Yes |
| Comma-separated data (.csv) | Yes | Yes |
| CSVY (CSV + YAML metadata header) (.csvy) | Yes | Yes |
| Feather R/Python interchange format (.feather) | Yes | Yes |
| Pipe-separated data (.psv) | Yes | Yes |
| Fixed-width format data (.fwf) | Yes | Yes |
| Serialized R objects (.rds) | Yes | Yes |
| Saved R objects (.RData) | Yes | Yes |
| JSON (.json) | Yes | Yes |
| YAML (.yml) | Yes | Yes |
| Stata (.dta) | Yes | Yes |
| SPSS (.sav) | Yes | Yes |
| SPSS Portable (.por) | Yes |  |
| "XBASE" database files (.dbf) | Yes | Yes |
| Excel (.xls) | Yes |  |
| Excel (.xlsx) | Yes | Yes |
| Weka Attribute-Relation File Format (.arff) | Yes | Yes |
| R syntax (.R) | Yes | Yes |
| Shallow XML documents (.xml) | Yes | Yes |
| HTML Tables (.html) | Yes | Yes |
| SAS (.sas7bdat) | Yes | Yes |
| SAS XPORT (.xpt) | Yes |  |
| Minitab (.mtp) | Yes |  |
| Epiinfo (.rec) | Yes |  |
| Systat (.syd) | Yes |  |
| Data Interchange Format (.dif) | Yes |  |
| OpenDocument Spreadsheet  (.ods) | Yes |  |
| Fortran data (no recognized extension) | Yes |  |
| Clipboard (default is tsv) | Yes (Mac and Windows) | Yes (Mac and Windows) |

Additionally, any format that is not supported by **rio** but that has a known R implementation will produce an informative error message pointing to a package and import or export function. Unrecognized formats will yield a simple "Unrecognized file format" error.

### Import ###

**rio** allows you to import files in almost any format using one, typically single-argument, function. `import` infers the file format from the file's extension and calls the appropriate data import function for you, returning a simple data.frame. This works for any for the formats listed above.

```{r, echo=FALSE, results='hide'}
library("rio")

export(mtcars, "mtcars.csv")
export(mtcars, "mtcars.rds")
export(mtcars, "mtcars.dta")
export(mtcars, "mtcars_noext", format = "csv")
```

```{r}
library("rio")

x <- import("mtcars.csv")
y <- import("mtcars.rds")
z <- import("mtcars.dta")

# confirm identical
all.equal(x, y, check.attributes = FALSE)
all.equal(x, z, check.attributes = FALSE)
```

If for some reason a file does not have an extension, or has a file extension that does not match its actual type, you can manually specify a file format to override the format inference step. For example, we can read in a CSV file that does not have a file extension by specifying `csv`:

```{r}
head(import("mtcars_noext", format = "csv"))
```


```{r, echo=FALSE, results='hide'}
unlink("mtcars.csv")
unlink("mtcars.rds")
unlink("mtcars.dta")
unlink("mtcars_noext")
```


### Export ###

The export capabilities of **rio** are somewhat more limited than the import capabilities, given the availability of different functions in various R packages and because import functions are often written to make use of data from other applications and it never seems to be a development priority to have functions to export to the formats used by other applications. That said, **rio** currently supports the following formats:


```{r}
library("rio")

export(mtcars, "mtcars.csv")
export(mtcars, "mtcars.rds")
export(mtcars, "mtcars.dta")
```

It is also easy to use `export` as part of an R pipeline (from magrittr or dplyr). For example, the following code uses `export` to save the results of a simple data transformation:

```{r}
library("magrittr")
mtcars %>% subset(hp > 100) %>%  aggregate(. ~ cyl + am, data = ., FUN = mean) %>% export(file = "mtcars2.dta")
```

```{r, echo=FALSE, results='hide'}
unlink("mtcars.csv")
unlink("mtcars.rds")
unlink("mtcars.dta")
unlink("mtcars2.dta")
```

## Convert ##

The `convert` function links `import` and `export` by constructing a dataframe from the imported file and immediately writing it back to disk. `convert` invisibly returns the file name of the exported file, so that it can be used to programmatically access the new file.

Because `convert` is just a thin wrapper for `import` and `export`, it is very easy to use. For example, we can convert 

```{r}
# create file to convert
export(mtcars, "mtcars.dta")

# convert Stata to SPSS
convert("mtcars.dta", "mtcars.sav")
```

`convert` also accepts lists of arguments for controlling import (`in_opts`) and export (`out_opts`). This can be useful for passing additional arguments to import or export methods. This could be useful, for example, for reading in a fixed-width format file and converting it to a comma-separated values file:

```{r}
# create an ambiguous file
fwf <- tempfile(fileext = ".fwf")
cat(file = fwf, "123456", "987654", sep = "\n")

# see two ways to read in the file
identical(import(fwf, widths = c(1,2,3)), import(fwf, widths = c(1,-2,3)))

# convert to CSV
convert(fwf, "fwf.csv", in_opts = list(widths = c(1,2,3)))
import("fwf.csv") # check conversion
```

```{r, echo=FALSE, results='hide'}
unlink("mtcars.dta")
unlink("mtcars.sav")
unlink("fwf.csv")
unlink(fwf)
```

It is also possible to use **rio** on the command-line by calling `Rscript` with the `-e` (expression) argument. For example, to convert a file from Stata (.dta) to comma-separated values (.csv), simply do the following:

```
Rscript -e "rio::convert('mtcars.dta', 'mtcars.csv')"
```